{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DragosTana/cv_homework/blob/main/Using_pre_trained_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5l5xxcxd0vpW"
      },
      "source": [
        "# Using pre-trained CNN\n",
        "\n",
        "In this lab, we will see:\n",
        "\n",
        "- Zero-shot performance of pre-trained backbone\n",
        "- Use pre-trained CNN as backbone\n",
        "- Fine-tuning the pre-trained CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tkm5SfEo1c1r"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Oh2yVOi3Lie"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "lr = 0.01\n",
        "epochs = 10\n",
        "device = torch.device(\"cuda\") # to use the GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWJUU9yp0vHp"
      },
      "outputs": [],
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "# create a split for train/validation. We can use early stop\n",
        "trainset, valset = torch.utils.data.random_split(dataset, [40000, 10000])\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2,\n",
        "                                          drop_last=True)\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
        "                                          shuffle=False, num_workers=2,\n",
        "                                          drop_last=False)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2,\n",
        "                                          drop_last=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load a pre-defined network with pretrained weights\n",
        "\n"
      ],
      "metadata": {
        "id": "mkiBrlvS1utb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uav1QxKwUXE5"
      },
      "outputs": [],
      "source": [
        "net = models.resnet18(pretrained=True)\n",
        "# override the fc layer of the network since it is of 1000 classes by default (ImageNet)\n",
        "net.fc = nn.Linear(512, 10)\n",
        "net.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWSNZ5Hf18Kn"
      },
      "outputs": [],
      "source": [
        "# count the trainable parameters of the model\n",
        "def count_trainable_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "count_trainable_parameters(net)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# frozen all the weights of the network, except for fc ones\n",
        "for param in net.parameters():\n",
        "    param.requires_grad = False\n",
        "net.fc.weight.requires_grad = True\n",
        "net.fc.bias.requires_grad = True\n",
        "count_trainable_parameters(net)"
      ],
      "metadata": {
        "id": "3uNOIDDs1_QD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UH7STI6m0dg9"
      },
      "outputs": [],
      "source": [
        "# define train and test function\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 500 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "        losses.append(loss.item())\n",
        "    return np.mean(losses)\n",
        "\n",
        "def test(model, device, test_loader, val=False):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    mode = \"Val\" if val else \"Test\"\n",
        "    print('\\{} set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        mode,\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    test_acc = correct / len(test_loader.dataset)\n",
        "    return test_loss, test_acc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=1e-04)"
      ],
      "metadata": {
        "id": "b3SqSMwo4qp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqA_UXQZ2zc8"
      },
      "outputs": [],
      "source": [
        "# the main loop\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "model_state_dict = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_loss = train(net, device, trainloader, optimizer, epoch)\n",
        "    train_losses.append(train_loss)\n",
        "    val_loss, val_acc = test(net, device, valloader)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iwFGBFz6FUH"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = test(net, device, testloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add additional layer to the pre-trained model\n"
      ],
      "metadata": {
        "id": "oo5XBPHC5ePQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fc1 = nn.Linear(512, 128)\n",
        "\n",
        "# Modify the existing fully connected layer (fc)\n",
        "net.fc = nn.Linear(128, 10)\n",
        "\n",
        "# Replace the model's classifier with a new sequential layer\n",
        "# that includes the new fc1 and the modified fc\n",
        "net.fc = nn.Sequential(\n",
        "    fc1,\n",
        "    nn.ReLU(),   # Optional: Add an activation function like ReLU\n",
        "    net.fc\n",
        ")\n",
        "net.to(device)"
      ],
      "metadata": {
        "id": "Z1UXzoW65drQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning some part of the CNN (not only the classifier)"
      ],
      "metadata": {
        "id": "otJejt0A1f9-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4nO6OAdXsXW"
      },
      "outputs": [],
      "source": [
        "# Unfreeze layer4 parameters\n",
        "for param in net.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Unfreeze fc layer parameters\n",
        "net.fc.requires_grad = True\n",
        "\n",
        "# Setting different learning rates\n",
        "layer4_params = {'params': net.layer4.parameters(), 'lr': 0.001}\n",
        "fc_params = {'params': net.fc.parameters(), 'lr': 0.1}\n",
        "\n",
        "# Assuming you are using an Adam optimizer\n",
        "optimizer = torch.optim.SGD([layer4_params, fc_params], momentum=0.9, weight_decay=1e-04)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1\n",
        "\n",
        "How many layers it is better to fine-tune?\n",
        "\n",
        "It is better to update all the weights of the model?"
      ],
      "metadata": {
        "id": "kyOs4mx92koz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2\n",
        "\n",
        "Try to change the hyper-parameters of the fine-tuning (e.g. lr of CNN layers and lr of the fc layers) and/or network architecture"
      ],
      "metadata": {
        "id": "cUunhER-2y9v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3\n",
        "\n",
        "Try to implement the model selection strategy (also known as early stopping) based on the validation accuracy on cifar10.\n",
        "\n",
        "Consider using the two following command to respectively save and load the state of all the parameters of the model in a moment."
      ],
      "metadata": {
        "id": "GuygmHB43UHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save all the parameters of the model\n",
        "model_state_dict = net.state_dict()\n",
        "\n",
        "# load saved weights on the model\n",
        "net.load_state_dict(model_state_dict)\n"
      ],
      "metadata": {
        "id": "uOhllTzr32Ld"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}